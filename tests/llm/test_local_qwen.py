import sys
import os
from pathlib import Path

# --- ç¯å¢ƒè·¯å¾„è®¾ç½® ---
# è¿™ä¸€æ­¥æ˜¯ä¸ºäº†è®© python èƒ½æ‰¾åˆ° src ç›®å½•ä¸‹çš„æ¨¡å—
# è·å–å½“å‰è„šæœ¬æ‰€åœ¨ç›®å½• (tests/llm)
current_dir = Path(__file__).parent
# è·å–é¡¹ç›®æ ¹ç›®å½• (å‡è®¾ tests åŒçº§ç›®å½• src å­˜åœ¨)
project_root = current_dir.parent.parent
sys.path.append(str(project_root))

from src.llm_service import LLMService
from langchain_core.messages import HumanMessage, SystemMessage

def test_qwen_connection():
    # 1. æŒ‡å®šé…ç½®æ–‡ä»¶è·¯å¾„ (å‡è®¾ä½ åœ¨é¡¹ç›®æ ¹ç›®å½•è¿è¡Œï¼Œæˆ–è€…æ˜¯ç»å¯¹è·¯å¾„)
    config_path = project_root / "config" / "llm_config.json"
    
    print(f"ğŸ“‚ è¯»å–é…ç½®æ–‡ä»¶: {config_path}")
    
    try:
        # 2. åˆå§‹åŒ–æœåŠ¡
        # æ³¨æ„ï¼šç¡®ä¿ä½ çš„ config/llm_config.json ä¸­ "selected_provider" æ˜¯ "Ollama"
        llm_service = LLMService(config_path=str(config_path))
        
        # 3. åˆ›å»º LLM
        llm = llm_service.create_llm()
        
        # 4. æ„é€ æµ‹è¯•æ¶ˆæ¯
        messages = [
            SystemMessage(content="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ— äººæœºæ§åˆ¶åŠ©æ‰‹ã€‚è¯·ç®€çŸ­å›ç­”ã€‚"),
            HumanMessage(content="ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±ï¼Œå¹¶å‘Šè¯‰æˆ‘ä½ èƒ½åšä»€ä¹ˆï¼Ÿ")
        ]
        
        print("\nğŸš€ å‘é€è¯·æ±‚ç»™ Ollama (Qwen3:8b)...")
        print("-" * 50)
        
        # 5. è°ƒç”¨æ¨¡å‹ (ä½¿ç”¨ invoke)
        response = llm.invoke(messages)
        
        print(response.content)
        print("-" * 50)
        print("âœ… æµ‹è¯•æˆåŠŸï¼LLM é€šä¿¡æ­£å¸¸ã€‚")

    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    test_qwen_connection()