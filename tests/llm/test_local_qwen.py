import sys
import os
from pathlib import Path

# --- ç¯å¢ƒè·¯å¾„è®¾ç½® ---
current_dir = Path(__file__).parent
project_root = current_dir.parent.parent
sys.path.append(str(project_root))

from src.llm_service import LLMService
from langchain_core.messages import HumanMessage, SystemMessage

def test_qwen_connection():
    config_path = project_root / "config" / "llm_config.json"
    print(f"ğŸ“‚ è¯»å–é…ç½®æ–‡ä»¶: {config_path}")
    
    try:
        # 1. åˆå§‹åŒ–æœåŠ¡
        llm_service = LLMService(config_path=str(config_path))
        
        # 2. æ˜¾å¼æŒ‡å®šè¦ä½¿ç”¨çš„ Provider
        target_provider = "Ollama" 
        # å¦‚æœä½ æƒ³æµ‹ DeepSeekï¼Œåªéœ€æ”¹ä¸º: target_provider = "DeepSeek"
        
        print(f"ğŸ‘‰ è¯·æ±‚åˆ›å»º Provider: {target_provider}")
        llm = llm_service.create_llm(target_provider)
        
        # 3. æ„é€ æµ‹è¯•æ¶ˆæ¯
        messages = [
            SystemMessage(content="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ— äººæœºæ§åˆ¶åŠ©æ‰‹ã€‚è¯·ç®€çŸ­å›ç­”ã€‚"),
            HumanMessage(content="ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±ã€‚")
        ]
        
        print(f"\nğŸš€ å‘é€è¯·æ±‚ç»™ {target_provider}...")
        print("-" * 50)
        
        # 4. è°ƒç”¨æ¨¡å‹
        response = llm.invoke(messages)
        
        print(response.content)
        print("-" * 50)
        print("âœ… æµ‹è¯•æˆåŠŸï¼LLM é€šä¿¡æ­£å¸¸ã€‚")

    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    test_qwen_connection()