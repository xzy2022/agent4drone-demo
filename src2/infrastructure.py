# src2/infrastructure.py
from typing import Optional
from langchain_ollama import ChatOllama
from langchain_openai import ChatOpenAI
from langchain_core.language_models.chat_models import BaseChatModel
from src2.configuration import SystemConfig

class LLMInfrastructure:
    def __init__(self, config: SystemConfig):
        self.config = config

    def create_llm(self, 
                   provider_name: str, 
                   model_name: Optional[str] = None, 
                   temperature: Optional[float] = None) -> BaseChatModel:
        """
        åˆ›å»º LLM å®ä¾‹
        :param provider_name: å¯¹åº”é…ç½®æ–‡ä»¶ä¸­çš„ key (å¦‚ "Ollama")
        :param model_name: å¼ºåˆ¶æŒ‡å®šæ¨¡å‹åç§° (å¦‚ "qwen2.5:1.5b")ï¼Œå¦‚æœä¸ä¼ åˆ™ä½¿ç”¨é…ç½®æ–‡ä»¶çš„é»˜è®¤å€¼
        :param temperature: è¦†ç›–æ¸©åº¦è®¾ç½®
        """
        # è·å–åŸºç¡€è¿æ¥é…ç½® (Base URL, API Key, Type)
        # æ³¨æ„ï¼šè¿™é‡Œç”¨ copy é˜²æ­¢ä¿®æ”¹ç¼“å­˜çš„é…ç½®
        llm_conf = self.config.get_llm_config(provider_name).copy()
        
        # === æ ¸å¿ƒé€»è¾‘ï¼šå‚æ•°ä¼˜å…ˆçº§ ===
        # è¿è¡Œæ—¶å‚æ•° > é…ç½®æ–‡ä»¶é»˜è®¤å€¼
        final_model = model_name if model_name else llm_conf.get("model")
        final_temp = temperature if temperature is not None else llm_conf.get("temperature", 0.1)

        llm_type = llm_conf.get("type", "").lower()
        
        print(f"ğŸ­ Init LLM: [{provider_name}] Model=[{final_model}] Temp=[{final_temp}]")

        if llm_type == "ollama":
            return ChatOllama(
                base_url=llm_conf.get("base_url"),
                model=final_model,  # ä½¿ç”¨æœ€ç»ˆå†³å®šçš„æ¨¡å‹å
                temperature=final_temp
            )
        elif llm_type == "openai":
            return ChatOpenAI(
                base_url=llm_conf.get("base_url"),
                api_key=llm_conf.get("api_key"),
                model=final_model,  # ä½¿ç”¨æœ€ç»ˆå†³å®šçš„æ¨¡å‹å
                temperature=final_temp,
                max_retries=2
            )
        else:
            raise ValueError(f"Unknown LLM type: {llm_type}")