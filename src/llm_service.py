# src\llm_service.py
import os
import json
import copy
from pathlib import Path
from typing import Dict, Any, Optional
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_ollama import ChatOllama


class LLMService:
    def __init__(self, config_path: str = "config/llm_config.json"):
        """
        åˆå§‹åŒ– LLM æœåŠ¡
        :param config_path: é…ç½®æ–‡ä»¶è·¯å¾„
        """
        load_dotenv()  # åŠ è½½ .env
        self.config_path = Path(config_path)
        self.raw_config = self._load_config()

    def _load_config(self) -> Dict[str, Any]:
        if not self.config_path.exists():
            raise FileNotFoundError(f"é…ç½®æ–‡ä»¶æœªæ‰¾åˆ°: {self.config_path.absolute()}")
        
        with open(self.config_path, 'r', encoding='utf-8') as f:
            return json.load(f)

    def _process_config(self, provider_name: str) -> Dict[str, Any]:
        """æå–æŒ‡å®š Provider çš„é…ç½®å¹¶å¤„ç†ç¯å¢ƒå˜é‡"""
        providers = self.raw_config.get("providers", {})
        provider_config = providers.get(provider_name)
        
        if not provider_config:
            valid_keys = list(providers.keys())
            raise ValueError(f"æœªæ‰¾åˆ° Provider '{provider_name}' çš„é…ç½®ã€‚å¯ç”¨é€‰é¡¹: {valid_keys}")

        # æ·±æ‹·è´ä»¥é˜²ä¿®æ”¹åŸå­—å…¸
        config = copy.deepcopy(provider_config)
        
        # æ›¿æ¢ç¯å¢ƒå˜é‡å ä½ç¬¦
        api_key = config.get("api_key")
        if isinstance(api_key, str) and api_key.startswith("${") and api_key.endswith("}"):
            env_var = api_key[2:-1]
            real_key = os.getenv(env_var)
            if not real_key and config.get("type") == "openai":
                print(f"âš ï¸ è­¦å‘Š: ç¯å¢ƒå˜é‡ {env_var} æœªè®¾ç½®ï¼ŒOpenAI å…¼å®¹æ¥å£å¯èƒ½è°ƒç”¨å¤±è´¥")
            config["api_key"] = real_key

        return config

    def create_llm(self, provider_name: str, override_temperature: Optional[float] = None):
        """
        æ ¹æ® provider_name åˆ›å»º LangChain å®ä¾‹
        :param provider_name: å¯¹åº”é…ç½®æ–‡ä»¶ä¸­ providers ä¸‹çš„ key (å¦‚ "Ollama", "DeepSeek")
        :param override_temperature: å¯é€‰ï¼Œè¦†ç›–é…ç½®æ–‡ä»¶ä¸­çš„æ¸©åº¦
        """
        conf = self._process_config(provider_name)
        
        llm_type = conf.get("type", "").lower()
        model_name = conf.get("model")
        temperature = override_temperature if override_temperature is not None else conf.get("temperature", 0.1)

        print(f"ğŸ”„ åˆå§‹åŒ– LLM: Provider=[{provider_name}] Type=[{llm_type}] Model=[{model_name}]")

        if llm_type == "ollama":
            from langchain_ollama import ChatOllama
            return ChatOllama(
                base_url=conf.get("base_url", "http://localhost:11434"),
                model=model_name,
                temperature=temperature
            )
        
        elif llm_type == "openai":
            from langchain_openai import ChatOpenAI

            return ChatOllama(
                base_url=conf.get("base_url"),
                model=model_name,
                temperature=temperature,
                # format="json",  # å¼ºåˆ¶ Ollama è¾“å‡º JSON (éœ€æ¨¡å‹æ”¯æŒï¼Œå¦‚ Llama3, Mistral)
                num_predict=200, # é™åˆ¶æœ€å¤§ token æ•°ï¼Œé˜²æ­¢ 100s çš„ç”Ÿæˆ
            )


        else:
            raise ValueError(f"ä¸æ”¯æŒçš„ LLM ç±»å‹: {llm_type}")

if __name__ == "__main__":
    # è‡ªæµ‹
    try:
        service = LLMService()
        # å¯ä»¥åœ¨è¿™é‡Œéšæ„åˆ‡æ¢ "Ollama" æˆ– "DeepSeek"
        llm = service.create_llm("Ollama")
        print("âœ… LLM å¯¹è±¡åˆ›å»ºæˆåŠŸ:", llm)
    except Exception as e:
        print(f"âŒ åˆå§‹åŒ–å¤±è´¥: {e}")