import os
import json
import copy
from pathlib import Path
from typing import Dict, Any, Optional
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_ollama import ChatOllama


class LLMService:
    def __init__(self, config_path: str = "config/llm_config.json"):
        """
        åˆå§‹åŒ– LLM æœåŠ¡
        :param config_path: é…ç½®æ–‡ä»¶è·¯å¾„
        """
        load_dotenv() # åŠ è½½ .env
        self.config_path = Path(config_path)
        self.full_config = self._load_config()
        self.active_config = self._get_active_config()

    def _load_config(self) -> Dict[str, Any]:
        if not self.config_path.exists():
            raise FileNotFoundError(f"é…ç½®æ–‡ä»¶æœªæ‰¾åˆ°: {self.config_path.absolute()}")
        
        with open(self.config_path, 'r', encoding='utf-8') as f:
            return json.load(f)

    def _get_active_config(self) -> Dict[str, Any]:
        """è·å–å¹¶è§£æå½“å‰é€‰ä¸­ provider çš„é…ç½®"""
        selected_name = self.full_config.get("selected_provider")
        if not selected_name:
            raise ValueError("é…ç½®æ–‡ä»¶ä¸­ç¼ºå°‘ 'selected_provider' å­—æ®µ")
        
        provider_config = self.full_config.get("providers", {}).get(selected_name)
        if not provider_config:
            raise ValueError(f"æœªæ‰¾åˆ° provider: {selected_name} çš„é…ç½®")

        # å¤„ç†é…ç½®ï¼ˆæ·±æ‹·è´ä»¥é˜²ä¿®æ”¹åŸå­—å…¸ï¼‰
        config = copy.deepcopy(provider_config)
        
        # æ ¸å¿ƒï¼šæ›¿æ¢ç¯å¢ƒå˜é‡å ä½ç¬¦
        api_key = config.get("api_key")
        if isinstance(api_key, str) and api_key.startswith("${") and api_key.endswith("}"):
            env_var = api_key[2:-1]
            real_key = os.getenv(env_var)
            if not real_key and config.get("type") == "openai":
                print(f"âš ï¸ è­¦å‘Š: ç¯å¢ƒå˜é‡ {env_var} æœªè®¾ç½®")
            config["api_key"] = real_key

        return config

    def create_llm(self):
        """
        åˆ›å»ºå¹¶è¿”å› LangChain çš„ ChatModel å®ä¾‹
        """
        conf = self.active_config
        llm_type = conf.get("type", "").lower()
        model_name = conf.get("model")
        temperature = conf.get("temperature", 0.1)

        print(f"ğŸ”„ åˆå§‹åŒ– LLM: [{llm_type}] {model_name}")

        if llm_type == "ollama":
            return ChatOllama(
                base_url=conf.get("base_url", "http://localhost:11434"),
                model=model_name,
                temperature=temperature
            )
        
        elif llm_type == "openai":
            return ChatOpenAI(
                base_url=conf.get("base_url"),
                api_key=conf.get("api_key"),
                model=model_name,
                temperature=temperature
            )
        
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„ LLM ç±»å‹: {llm_type}")

# æ–¹ä¾¿å¤–éƒ¨ç›´æ¥è°ƒç”¨çš„å•ä¾‹æ¨¡å¼ï¼ˆå¯é€‰ï¼‰
if __name__ == "__main__":
    # ç®€å•çš„è‡ªæµ‹
    service = LLMService()
    llm = service.create_llm()
    print("LLM å¯¹è±¡åˆ›å»ºæˆåŠŸ:", llm)